{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Notes on this Notebook file**\n",
    "\n",
    "Bacause of my lack of computing skill, I bulit my computational process via `.ipynb` format rather than to build executable Python script `run.med.sh`.\n",
    "\n",
    "This notebook was written under the conditions below:\n",
    "- Cloning GitHub repository: `git clone 'http://github.com/11775website/11775-hws'` was done on `/home/ubuntu` directory\n",
    "- Downloading source data: was done on `/home/ubuntu` directory\n",
    "- Raw feature generation (MFCC / ASRS) : was done on `/home/ubuntu` directory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import lightgbm\n",
    "import time\n",
    "import os\n",
    "\n",
    "from glob import glob\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics.pairwise import chi2_kernel, laplacian_kernel\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import average_precision_score\n",
    "from sklearn.preprocessing import normalize\n",
    "from sklearn.feature_extraction import text\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MFCC feature extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Path setting for MFCC raw data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = \"/home/ubuntu/mfcc\"\n",
    "EXT = \"*.csv\"\n",
    "filelist = np.array([file for file in glob(os.path.join(PATH, EXT))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "filelist_simple = np.array([file[37:44] for file in glob(os.path.join(PATH, EXT))])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**K-means clustering for feature extraction**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sample = pd.read_csv('select.mfcc.csv', header=None, sep=';')\n",
    "start_time = time.time()\n",
    "n_clusters = 70\n",
    "n_init = 5\n",
    "kmeans = KMeans(n_clusters = n_clusters, random_state = 0, n_init = n_init, n_jobs = -1).fit(train_sample)\n",
    "\n",
    "def get_features(k, model, path_list):\n",
    "    loaded_model= model\n",
    "    start_time = time.time()\n",
    "    features_dict = dict()\n",
    "    filelist = path_list\n",
    "    for i in range(len(filelist)):\n",
    "        data = pd.read_csv(filelist[i], sep = ';', header = None)\n",
    "        pred_centers = loaded_model.predict(data)\n",
    "        num_clusters = k\n",
    "        bow_preds = np.zeros((1, num_clusters))\n",
    "\n",
    "        for ind in pred_centers:\n",
    "            bow_preds[0, ind] += 1\n",
    "        norm_feat = (1.0 * bow_preds)/np.sum(bow_preds)\n",
    "        features_dict[i] = pd.DataFrame(norm_feat)\n",
    "\n",
    "    features_total = features_dict[0].copy()\n",
    "    for i in range(1, len(features_dict)):\n",
    "        foo = features_dict[i].copy()\n",
    "        features_total = pd.concat([features_total, foo], axis = 0)\n",
    "        features_total = features_total.reset_index().drop('index', axis = 1)\n",
    "        \n",
    "    return features_total\n",
    "\n",
    "total_features = get_features(70, model = kmeans, path_list = filelist)\n",
    "total_features.to_csv(r'features_kmeans.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = pd.read_csv('features_kmeans.csv').drop(columns = ['Unnamed: 0']).to_numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Splitting dataset (train / val / test)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_PATH = '/home/ubuntu/11775-hws/all_trn.lst'\n",
    "VAL_PATH = '/home/ubuntu/11775-hws/all_val.lst'\n",
    "TEST_PATH = '/home/ubuntu/11775-hws/all_test_fake.lst'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_name, train_label = [], []\n",
    "val_name, val_label = [], []\n",
    "test_name = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(TRAIN_PATH, 'r') as f:\n",
    "    for line in f:\n",
    "        tmp = line.split(' ')\n",
    "        train_name.append(tmp[0])\n",
    "        train_label.append(tmp[1][:-1])\n",
    "train_name = np.array(train_name)\n",
    "train_label = np.array(train_label)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(VAL_PATH, 'r') as f:\n",
    "    for line in f:\n",
    "        tmp = line.split(' ')\n",
    "        val_name.append(tmp[0])\n",
    "        val_label.append(tmp[1][:-1])\n",
    "val_name = np.array(val_name)\n",
    "val_label = np.array(val_label)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(TEST_PATH, 'r') as f:\n",
    "    for line in f:\n",
    "        tmp = line.split(' ')\n",
    "        test_name.append(tmp[0])\n",
    "test_name = np.array(test_name)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = []\n",
    "zeros = [0] * 70\n",
    "for name in train_name:\n",
    "    if name not in filelist_simple:\n",
    "        tmp = np.argwhere(train_name == name)[0][0]\n",
    "        train_label[tmp] = 'NULL'\n",
    "        train_data.append(zeros)\n",
    "    elif name in filelist_simple:\n",
    "        train_data.append(features[np.argwhere(filelist_simple == name)[0][0]])\n",
    "train_data = np.array(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_data = []\n",
    "zeros = [0] * 70\n",
    "for name in val_name:\n",
    "    if name not in filelist_simple:\n",
    "        tmp = np.argwhere(val_name == name)[0][0]\n",
    "        val_label[tmp] = 'NULL'\n",
    "        val_data.append(zeros)\n",
    "    elif name in filelist_simple:\n",
    "        val_data.append(features[np.argwhere(filelist_simple == name)[0][0]])\n",
    "val_data = np.array(val_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = []\n",
    "zeros = [0] * 70\n",
    "for name in test_name:\n",
    "    if name not in filelist_simple:\n",
    "        test_data.append(zeros)\n",
    "    elif name in filelist_simple:\n",
    "        test_data.append(features[np.argwhere(filelist_simple == name)[0][0]])\n",
    "test_data = np.array(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_label_001 = train_label.copy()\n",
    "for i in range(train_label.shape[0]):\n",
    "    if train_label[i] != 'P001':\n",
    "        train_label_001[i] = 'NULL'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_label_002 = train_label.copy()\n",
    "for i in range(train_label.shape[0]):\n",
    "    if train_label[i] != 'P002':\n",
    "        train_label_002[i] = 'NULL'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_label_003 = train_label.copy()\n",
    "for i in range(train_label.shape[0]):\n",
    "    if train_label[i] != 'P003':\n",
    "        train_label_003[i] = 'NULL'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_label_001 = val_label.copy()\n",
    "for i in range(val_label.shape[0]):\n",
    "    if val_label[i] != 'P001':\n",
    "        val_label_001[i] = 'NULL'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_label_002 = val_label.copy()\n",
    "for i in range(val_label.shape[0]):\n",
    "    if val_label[i] != 'P002':\n",
    "        val_label_002[i] = 'NULL'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_label_003 = val_label.copy()\n",
    "for i in range(val_label.shape[0]):\n",
    "    if val_label[i] != 'P003':\n",
    "        val_label_003[i] = 'NULL'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LightGBM classifier (MFCC feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.19325615727695478"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lgbm_001 = LGBMClassifier(random_state = 0, n_jobs = -1, class_weight = 'balanced',\n",
    "                      boosting_type = 'goss', n_estimators = 500, learning_rate = 0.002,\n",
    "                      reg_alpha = 0.5, reg_beta = 0.5, max_depth = 30)\n",
    "lgbm_001.fit(train_data, train_label_001)\n",
    "val_prob_001 = lgbm_001.predict_proba(val_data).T\n",
    "average_precision_score(y_true = val_label_001, y_score = val_prob_001[1], pos_label = 'P001')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4185909278600208"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lgbm_002 = LGBMClassifier(random_state = 0, n_jobs = -1, class_weight = 'balanced',\n",
    "                      boosting_type = 'goss', n_estimators = 500, learning_rate = 0.002,\n",
    "                      reg_alpha = 0.5, reg_beta = 0.5, max_depth = 30)\n",
    "lgbm_002.fit(train_data, train_label_002)\n",
    "val_prob_002 = lgbm_002.predict_proba(val_data).T\n",
    "average_precision_score(y_true = val_label_002, y_score = val_prob_002[1], pos_label = 'P002')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.22571787956680175"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lgbm_003 = LGBMClassifier(random_state = 0, n_jobs = -1, class_weight = 'balanced',\n",
    "                      boosting_type = 'goss', n_estimators = 500, learning_rate = 0.002,\n",
    "                      reg_alpha = 0.5, reg_beta = 0.5, max_depth = 30)\n",
    "lgbm_003.fit(train_data, train_label_003)\n",
    "val_prob_003 = lgbm_003.predict_proba(val_data).T\n",
    "average_precision_score(y_true = val_label_003, y_score = val_prob_003[1], pos_label = 'P003')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ASRS feature extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Path setting for ASRS raw data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH_ASRS = '/home/ubuntu/asrs/*.txt'\n",
    "filelist_asrs = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**import raw ASRS data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "for file in glob(PATH_ASRS):\n",
    "    filelist_asrs.append(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "filelist_asrs_simple = np.array([name[18:25] for name in filelist_asrs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def concatenate_list_data(list):\n",
    "    result= ''\n",
    "    for element in list:\n",
    "        result += str(element)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_asrs = []\n",
    "for i in range(len(filelist_asrs)):\n",
    "    with open (filelist_asrs[i], \"r\") as myfile:\n",
    "        data = myfile.readlines()\n",
    "        data = concatenate_list_data(data)\n",
    "    text_asrs.append(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Generating BoW features**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "vect = CountVectorizer(stop_words=\"english\")\n",
    "bow = vect.fit_transform(text_asrs).toarray()\n",
    "norm_bow = normalize(bow, norm = 'l1', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2226, 6986)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "norm_bow.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Splitting dataset (train / val / test)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "asrs_train_label = train_label.copy()\n",
    "asrs_val_label = val_label.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "asrs_train_data = []\n",
    "zeros = [0] * 6986\n",
    "for name in train_name:\n",
    "    if name not in filelist_asrs_simple:\n",
    "        tmp = np.argwhere(train_name == name)[0][0]\n",
    "        asrs_train_label[tmp] = 'NULL'\n",
    "        asrs_train_data.append(zeros)\n",
    "    elif name in filelist_asrs_simple:\n",
    "        asrs_train_data.append(norm_bow[np.argwhere(filelist_asrs_simple == name)[0][0]])\n",
    "asrs_train_data = np.array(asrs_train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "asrs_val_data = []\n",
    "zeros = [0] * 6986\n",
    "for name in val_name:\n",
    "    if name not in filelist_asrs_simple:\n",
    "        tmp = np.argwhere(val_name == name)[0][0]\n",
    "        asrs_val_label[tmp] = 'NULL'\n",
    "        asrs_val_data.append(zeros)\n",
    "    elif name in filelist_asrs_simple:\n",
    "        asrs_val_data.append(norm_bow[np.argwhere(filelist_asrs_simple == name)[0][0]])\n",
    "asrs_val_data = np.array(asrs_val_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "asrs_test_data = []\n",
    "zeros = [0] * 6986\n",
    "for name in test_name:\n",
    "    if name not in filelist_asrs_simple:\n",
    "        asrs_test_data.append(zeros)\n",
    "    elif name in filelist_asrs_simple:\n",
    "        asrs_test_data.append(norm_bow[np.argwhere(filelist_asrs_simple == name)[0][0]])\n",
    "asrs_test_data = np.array(asrs_test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "asrs_train_label_001 = asrs_train_label.copy()\n",
    "for i in range(asrs_train_label.shape[0]):\n",
    "    if asrs_train_label[i] != 'P001':\n",
    "        asrs_train_label_001[i] = 'NULL'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "asrs_train_label_002 = asrs_train_label.copy()\n",
    "for i in range(asrs_train_label.shape[0]):\n",
    "    if asrs_train_label[i] != 'P002':\n",
    "        asrs_train_label_002[i] = 'NULL'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "asrs_train_label_003 = asrs_train_label.copy()\n",
    "for i in range(asrs_train_label.shape[0]):\n",
    "    if asrs_train_label[i] != 'P003':\n",
    "        asrs_train_label_003[i] = 'NULL'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "asrs_val_label_001 = asrs_val_label.copy()\n",
    "for i in range(asrs_val_label.shape[0]):\n",
    "    if asrs_val_label[i] != 'P001':\n",
    "        asrs_val_label_001[i] = 'NULL'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "asrs_val_label_002 = asrs_val_label.copy()\n",
    "for i in range(asrs_val_label.shape[0]):\n",
    "    if asrs_val_label[i] != 'P002':\n",
    "        asrs_val_label_002[i] = 'NULL'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "asrs_val_label_003 = asrs_val_label.copy()\n",
    "for i in range(asrs_val_label.shape[0]):\n",
    "    if asrs_val_label[i] != 'P003':\n",
    "        asrs_val_label_003[i] = 'NULL'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LightGBM classifier (ASRS features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.08172935069886589"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "asrs_lgbm_001 = LGBMClassifier(random_state = 0, n_jobs = -1, class_weight = 'balanced',\n",
    "                      boosting_type = 'goss', n_estimators = 10000, learning_rate = 0.002,\n",
    "                      reg_alpha = 0.5, reg_beta = 0.5, max_depth = 30)\n",
    "asrs_lgbm_001.fit(asrs_train_data, asrs_train_label_001)\n",
    "asrs_val_prob_001 = asrs_lgbm_001.predict_proba(asrs_val_data).T\n",
    "average_precision_score(y_true = asrs_val_label_001, y_score = asrs_val_prob_001[1], pos_label = 'P001')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.03220559678193491"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "asrs_lgbm_002 = LGBMClassifier(random_state = 0, n_jobs = -1, class_weight = 'balanced',\n",
    "                      boosting_type = 'goss', n_estimators = 10000, learning_rate = 0.002,\n",
    "                      reg_alpha = 0.5, reg_beta = 0.5, max_depth = 30)\n",
    "asrs_lgbm_002.fit(asrs_train_data, asrs_train_label_002)\n",
    "asrs_val_prob_002 = asrs_lgbm_002.predict_proba(asrs_val_data).T\n",
    "average_precision_score(y_true = asrs_val_label_002, y_score = asrs_val_prob_002[1], pos_label = 'P002')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.13441639081809278"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "asrs_lgbm_003 = LGBMClassifier(random_state = 0, n_jobs = -1, class_weight = 'balanced',\n",
    "                      boosting_type = 'goss', n_estimators = 500, learning_rate = 0.002,\n",
    "                      reg_alpha = 0.5, reg_beta = 0.5, max_depth = 30)\n",
    "asrs_lgbm_003.fit(asrs_train_data, asrs_train_label_003)\n",
    "asrs_val_prob_003 = asrs_lgbm_003.predict_proba(asrs_val_data).T\n",
    "average_precision_score(y_true = asrs_val_label_003, y_score = asrs_val_prob_003[1], pos_label = 'P003')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Score Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "P001_mfcc = lgbm_001.predict_proba(test_data).T[1]\n",
    "P002_mfcc = lgbm_002.predict_proba(test_data).T[1]\n",
    "P003_mfcc = lgbm_003.predict_proba(test_data).T[1]\n",
    "P001_asrs = asrs_lgbm_001.predict_proba(asrs_test_data).T[1]\n",
    "P002_asrs = asrs_lgbm_002.predict_proba(asrs_test_data).T[1]\n",
    "P003_asrs = asrs_lgbm_003.predict_proba(asrs_test_data).T[1]\n",
    "P001_best = lgbm_001.predict_proba(test_data).T[1]\n",
    "P002_best = lgbm_002.predict_proba(test_data).T[1]\n",
    "P003_best = lgbm_003.predict_proba(test_data).T[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "names = ['P001_mfcc.lst', 'P002_mfcc.lst', 'P003_mfcc.lst',\n",
    "         'P001_asrs.lst', 'P002_asrs.lst', 'P003_asrs.lst',\n",
    "         'P001_best.lst', 'P002_best.lst', 'P003_best.lst']\n",
    "scores = [P001_mfcc, P002_mfcc, P003_mfcc,\n",
    "         P001_asrs, P002_asrs, P003_asrs,\n",
    "         P001_best, P002_best, P003_best]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx in range(len(names)):\n",
    "    with open(names[idx], 'w') as f:\n",
    "        for score in scores[idx]:\n",
    "            f.write(\"{}\\n\".format(str(score)))\n",
    "        f.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
